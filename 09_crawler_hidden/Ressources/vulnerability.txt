CRAWLER HIDDEN

In the robots.txt we can see a .hidden repository
If we give a look, it contains other repositories and so on and so on
So let's download the entire website and check it out:
	wget --convert-links -np -r --level 4 -e robots=off http://192.168.1.59/.hidden/
Then I tried a recursive grep with the keyword flag but it didnt give any result
If you check a few README files you notice that they tend to repeat themselves
So I wrote a script to display all distinct README files:
	python3 directory_search.py 192.168.1.59/.hidden/
		=> We get the flag

PROTECTION
	Putting tons of files just to hide one is not a very good idea
	Create a htaccess or something like this
